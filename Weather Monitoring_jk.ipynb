{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c84567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tarfile\n",
    "import gzip\n",
    "import re\n",
    "import os\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ffe305",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_num = 4 # Number of past years to consider\n",
    "extremes_num = 10 # Number of hottest and coldest places to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc00a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearfiles = os.listdir(\"./gsod_all_years\")\n",
    "yearfiles.sort()\n",
    "yearfiles = yearfiles[-year_num:]\n",
    "years = [int(re.findall('\\d+',yearfile)[0]) for yearfile in yearfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479b9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_loc = pd.read_csv('isd-history.csv')\n",
    "station_loc = station_loc.replace([0.0, -999.0, -999.9],np.nan)\n",
    "station_loc = station_loc[pd.notnull(station_loc['LAT']) & pd.notnull(station_loc['LON'])]\n",
    "station_loc = station_loc[[int(re.findall('^\\d{4}', str(end_year))[0])==max(years) for end_year in station_loc['END']]]\n",
    "station_loc = station_loc[[int(re.findall('^\\d{4}', str(beg_year))[0])<=min(years) for beg_year in station_loc['BEGIN']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99ca7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_loc['LBL'] = station_loc[['STATION NAME','STATE','CTRY']].apply(lambda x: x.str.cat(sep=', '), axis=1)\n",
    "station_loc['ELEV_LBL'] = station_loc['ELEV(M)'].apply(lambda x: 'Elevation: '+str(x)+' m' if ~np.isnan(x) else np.nan)\n",
    "station_loc['LBL'] = station_loc[['LBL','ELEV_LBL']].apply(lambda x: x.str.cat(sep='<br>'), axis=1)\n",
    "# station_loc = station_loc.drop(['STATION NAME','STATE','ELEV_LBL','ICAO','BEGIN','END'], axis=1)\n",
    "station_loc = station_loc.drop(['ELEV_LBL','ICAO','BEGIN','END'], axis=1)\n",
    "#station_loc = station_loc.sample(stat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f0a4ba-5220-4934-b31e-299a94206ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find US Stations\n",
    "station_loc.head()\n",
    "us_stations = station_loc.loc[station_loc['CTRY'] == 'US', :]\n",
    "us_stations_list = us_stations['USAF'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984cd629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([])\n",
    "df_day = pd.DataFrame([])\n",
    "\n",
    "def preprocess_station_file_content(content):\n",
    "    headers=content.pop(0)\n",
    "    headers=[headers[ind] for ind in [0,1,2,3,4,8,11,12]]\n",
    "    for d in range(len(content)):\n",
    "        content[d]=[content[d][ind] for ind in [0,1,2,3,5,13,17,18]]\n",
    "    content=pd.DataFrame(content, columns=headers)\n",
    "    content.rename(columns={'STN---': 'USAF'}, inplace=True)\n",
    "    content['MAX'] = content['MAX'].apply(lambda x: re.sub(\"\\*$\",\"\",x))\n",
    "    content['MIN'] = content['MIN'].apply(lambda x: re.sub(\"\\*$\",\"\",x))\n",
    "    content[['WBAN','TEMP','DEWP','WDSP','MAX','MIN']] = content[['WBAN','TEMP','DEWP','WDSP','MAX','MIN']].apply(pd.to_numeric)\n",
    "    content['YEARMODA']=pd.to_datetime(content['YEARMODA'], format='%Y%m%d', errors='ignore')\n",
    "    content['YEAR']=pd.DatetimeIndex(content['YEARMODA']).year\n",
    "    content['MONTH']=pd.DatetimeIndex(content['YEARMODA']).month\n",
    "    content['DAY']=pd.DatetimeIndex(content['YEARMODA']).day\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961fcc4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yearfile = yearfiles[-1]\n",
    "# print(yearfile)\n",
    "# i=0\n",
    "# tar = tarfile.open(\"./gsod_all_years/\"+yearfile, \"r\")\n",
    "# print(len(tar.getmembers()[1:]))\n",
    "# #for member in np.random.choice(tar.getmembers()[1:], size=stat_num, replace=False):\n",
    "# for member in tar.getmembers()[1:]:\n",
    "#     name_parts = re.sub(\"\\.op\\.gz$\",\"\",re.sub(\"^\\./\",\"\",member.name)).split(\"-\")\n",
    "#     usaf = name_parts[0]\n",
    "#     wban = int(name_parts[1])\n",
    "#     if station_loc[(station_loc['USAF']==usaf) & (station_loc['WBAN']==wban)].shape[0]!=0:\n",
    "#         i=i+1\n",
    "#         #if i%(stat_num//10) == 0: print(i)\n",
    "#         f=tar.extractfile(member)\n",
    "#         f=gzip.open(f, 'rb')\n",
    "#         content=[re.sub(\" +\", \",\", line.decode(\"utf-8\")).split(\",\") for line in f.readlines()]\n",
    "#         content=preprocess_station_file_content(content)\n",
    "#         df_day = df_day.append(content[content['YEARMODA']==content['YEARMODA'].max()])\n",
    "#         content = content.groupby(['USAF','WBAN','YEAR','MONTH']).agg('median').reset_index()\n",
    "#         df = df.append(content)\n",
    "# tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2e43a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84123525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7addd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(df, station_loc, how=\"right\", on=\"USAF\")\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afedaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_clean_df = merged_df.loc[merged_df[\"USAF\"] != \"999999\", :]\n",
    "# merged_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bddaa607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_clean_df.to_csv('merged_cleaned_dataframe.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6b6e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day = df_day['YEARMODA'].max()\n",
    "# df_day = df_day[df_day['YEARMODA']==day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a46bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsod_2016.tar\n",
      "12076\n",
      "gsod_2017.tar\n",
      "11923\n"
     ]
    }
   ],
   "source": [
    "for yearfile in yearfiles[:-1]:\n",
    "    print(yearfile)\n",
    "    i=0\n",
    "    tar = tarfile.open(\"./gsod_all_years/\"+yearfile, \"r\")\n",
    "    print(len(tar.getmembers()[1:]))\n",
    "    #for member in np.random.choice(tar.getmembers()[1:], size=stat_num, replace=False):\n",
    "    for member in tar.getmembers()[1:]:\n",
    "        name_parts = re.sub(\"\\.op\\.gz$\",\"\",re.sub(\"^\\./\",\"\",member.name)).split(\"-\")\n",
    "        usaf = name_parts[0]\n",
    "        wban = int(name_parts[1])\n",
    "        if station_loc[(station_loc['USAF']==usaf) & (station_loc['WBAN']==wban)].shape[0]!=0 and (usaf in us_stations_list) and (usaf != 999999):\n",
    "            i=i+1\n",
    "            #if i%(stat_num//10) == 0: print(i)\n",
    "            f=tar.extractfile(member)\n",
    "            f=gzip.open(f, 'rb')\n",
    "            content=[re.sub(\" +\", \",\", line.decode(\"utf-8\")).split(\",\") for line in f.readlines()]\n",
    "            content=preprocess_station_file_content(content)\n",
    "#             df_day = df_day.append(content[content['YEARMODA']==content['YEARMODA'].max()])\n",
    "            df_day = df_day.append(content, ignore_index=True)\n",
    "            content = content.groupby(['USAF','WBAN','YEAR','MONTH']).agg('median').reset_index()\n",
    "            df = df.append(content)\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12846aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28a67d-7e60-4ff8-8a21-f19b77421415",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_loc['WBAN'] = station_loc['WBAN'].astype(float)\n",
    "station_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_day, station_loc, how=\"outer\", on=\"USAF\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576c72b-43f2-4673-a929-a45413a0816b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clean_df = merged_df.loc[merged_df['MIN'] != 9999.9]\n",
    "merged_clean_df = merged_clean_df.loc[merged_clean_df['MAX'] != 9999.9]\n",
    "merged_clean_df = merged_clean_df.loc[merged_clean_df['TEMP'] != 9999.9]\n",
    "merged_clean_df = merged_clean_df.loc[merged_clean_df[\"USAF\"] != \"999999\", :]\n",
    "merged_clean_df.dropna(how='any')\n",
    "us_merged_df = merged_clean_df.loc[merged_clean_df[\"CTRY\"] == \"US\", :]\n",
    "us_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=us_merged_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# us_drop_na_df = us_merged_df.dropna(how='any')\n",
    "# us_drop_na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2cadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (new_df['WBAN_x'].equals(new_df['WBAN_y'])):\n",
    "    new_df = new_df.drop(['WBAN_y'], axis=1)\n",
    "    new_df = new_df.rename(columns={\"WBAN_x\": \"WBAN\"})\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08281bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b589b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# us_drop_na_df['YEARMODA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a07b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(us_drop_na_df['YEARMODA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f882a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(new_df['YEARMODA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab60b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04678829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a754f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.USWeather\n",
    "collection = db.weather\n",
    "db.collection.insert_many(new_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_agg_df = pd.merge(df, station_loc, how=\"outer\", on=\"USAF\")\n",
    "merged_agg_df\n",
    "merged_clean_agg_df = merged_agg_df.loc[merged_agg_df['MIN'] != 9999.9]\n",
    "merged_clean_agg_df = merged_clean_agg_df.loc[merged_clean_agg_df['MAX'] != 9999.9]\n",
    "merged_clean_agg_df = merged_clean_agg_df.loc[merged_clean_agg_df['TEMP'] != 9999.9]\n",
    "merged_clean_agg_df = merged_clean_agg_df.loc[merged_clean_agg_df[\"USAF\"] != \"999999\", :]\n",
    "merged_clean_agg_df.dropna(how='any')\n",
    "us_merged_agg_df = merged_clean_agg_df.loc[merged_clean_agg_df[\"CTRY\"] == \"US\", :]\n",
    "us_merged_agg_df\n",
    "new_agg_df=us_merged_agg_df.dropna(how='any')\n",
    "if (new_agg_df['WBAN_x'].equals(new_agg_df['WBAN_y'])):\n",
    "    new_agg_df = new_agg_df.drop(['WBAN_y'], axis=1)\n",
    "    new_agg_df = new_agg_df.rename(columns={\"WBAN_x\": \"WBAN\"})\n",
    "new_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285896ab-d215-4895-8321-8a9e3e7f113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a timestamp to the beginning of the month for aggregates\n",
    "# https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns-apply-a-function-o\n",
    "# https://www.w3schools.com/python/python_datetime.asp\n",
    "def add_aggregate_timestamp(row):\n",
    "    return dt.datetime(int(row['YEAR']), int(row['MONTH']), 1)\n",
    "\n",
    "new_agg_df['YEARMODA'] = new_agg_df.apply(lambda row: add_aggregate_timestamp(row), axis=1)\n",
    "new_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d639d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.USWeatherAgg\n",
    "collection = db.weather\n",
    "db.collection.insert_many(new_agg_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be36b6c-50cf-47d5-a379-5937ac61ba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72943f-6069-4cc8-808b-52775dbd25c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5917621-d2b9-4573-91c5-a932306174cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
